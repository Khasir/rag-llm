{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with RAG\n",
    "Following this guide: https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/notebooks/pe-rag.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaz/repos/sophia/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fireworks.client\n",
    "import dotenv\n",
    "import chromadb\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "config = dotenv.dotenv_values(\".env\")\n",
    "\n",
    "fireworks.client.api_key = config['FIREWORKS_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=None, max_tokens=50):\n",
    "\n",
    "    fw_model_dir = \"accounts/fireworks/models/\"\n",
    "\n",
    "    if model is None:\n",
    "        model = fw_model_dir + \"llama-v2-7b\"\n",
    "    else:\n",
    "        model = fw_model_dir + model\n",
    "\n",
    "    completion = fireworks.client.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Katie and I am a 20 year old student at the University of Leeds. I am currently studying a BA in English Literature and Creative Writing. I have been working as a tutor for over 3 years now and I'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"Hello, my name is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [Your Name]. I am a [Your Profession/Occupation]. I am writing to [Purpose of Writing].\\n\\nI am writing to [Purpose of Writing] because [Reason for Writing]. I believe that ['"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_llm = \"mistral-7b-instruct-4k\"\n",
    "\n",
    "get_completion(\"Hello, my name is\", model=mistral_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".\\n1. Why don't scientists trust atoms? Because they make up everything!\\n2. Did you hear about the mathematician who’s afraid of negative numbers? He will stop at nothing to avoid them.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_llm = \"mistral-7b-instruct-4k\"\n",
    "\n",
    "get_completion(\"Tell me 2 jokes\", model=mistral_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dear John Doe,\n",
      "\n",
      "We, Tom and Mary, would like to extend our heartfelt gratitude for your attendance at our wedding. It was a pleasure to have you there, and we truly appreciate the effort you made to be a part of our special day.\n",
      "\n",
      "We were thrilled to learn about your fun fact - climbing Mount Everest is an incredible accomplishment! We hope you had a safe and memorable journey.\n",
      "\n",
      "Thank you again for joining us on this special occasion. We hope to stay in touch and catch up on all the amazing things you've been up to.\n",
      "\n",
      "With love,\n",
      "\n",
      "Tom and Mary\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"[INST]\n",
    "Given the following wedding guest data, write a very short 3-sentences thank you letter:\n",
    "\n",
    "{\n",
    "  \"name\": \"John Doe\",\n",
    "  \"relationship\": \"Bride's cousin\",\n",
    "  \"hometown\": \"New York, NY\",\n",
    "  \"fun_fact\": \"Climbed Mount Everest in 2020\",\n",
    "  \"attending_with\": \"Sophia Smith\",\n",
    "  \"bride_groom_name\": \"Tom and Mary\"\n",
    "}\n",
    "\n",
    "Use only the data provided in the JSON object above.\n",
    "\n",
    "The senders of the letter is the bride and groom, Tom and Mary.\n",
    "[/INST]\"\"\"\n",
    "\n",
    "response = get_completion(prompt, model=mistral_llm, max_tokens=150)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Use Case: Generating Short Paper Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/dair-ai/ML-Papers-of-the-Week/tree/main/research\n",
    "ml_papers = pd.read_csv(\"./data/ml-potw-10232023.csv\", header=0)\n",
    "\n",
    "# remove rows with empty titles or descriptions\n",
    "ml_papers = ml_papers.dropna(subset=[\"Title\", \"Description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>PaperURL</th>\n",
       "      <th>TweetURL</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llemma</td>\n",
       "      <td>an LLM for mathematics which is based on conti...</td>\n",
       "      <td>https://arxiv.org/abs/2310.10631</td>\n",
       "      <td>https://x.com/zhangir_azerbay/status/171409802...</td>\n",
       "      <td>We present Llemma, a large language model for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLMs for Software Engineering</td>\n",
       "      <td>a comprehensive survey of LLMs for software en...</td>\n",
       "      <td>https://arxiv.org/abs/2310.03533</td>\n",
       "      <td>https://x.com/omarsar0/status/1713940983199506...</td>\n",
       "      <td>This paper provides a survey of the emerging a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Self-RAG</td>\n",
       "      <td>presents a new retrieval-augmented framework t...</td>\n",
       "      <td>https://arxiv.org/abs/2310.11511</td>\n",
       "      <td>https://x.com/AkariAsai/status/171511027707796...</td>\n",
       "      <td>Despite their remarkable capabilities, large l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Retrieval-Augmentation for Long-form Question ...</td>\n",
       "      <td>explores retrieval-augmented language models o...</td>\n",
       "      <td>https://arxiv.org/abs/2310.12150</td>\n",
       "      <td>https://x.com/omarsar0/status/1714986431859282...</td>\n",
       "      <td>We present a study of retrieval-augmented lang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GenBench</td>\n",
       "      <td>presents a framework for characterizing and un...</td>\n",
       "      <td>https://www.nature.com/articles/s42256-023-007...</td>\n",
       "      <td>https://x.com/AIatMeta/status/1715041427283902...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                                             Llemma   \n",
       "1                      LLMs for Software Engineering   \n",
       "2                                           Self-RAG   \n",
       "3  Retrieval-Augmentation for Long-form Question ...   \n",
       "4                                           GenBench   \n",
       "\n",
       "                                         Description  \\\n",
       "0  an LLM for mathematics which is based on conti...   \n",
       "1  a comprehensive survey of LLMs for software en...   \n",
       "2  presents a new retrieval-augmented framework t...   \n",
       "3  explores retrieval-augmented language models o...   \n",
       "4  presents a framework for characterizing and un...   \n",
       "\n",
       "                                            PaperURL  \\\n",
       "0                   https://arxiv.org/abs/2310.10631   \n",
       "1                   https://arxiv.org/abs/2310.03533   \n",
       "2                   https://arxiv.org/abs/2310.11511   \n",
       "3                   https://arxiv.org/abs/2310.12150   \n",
       "4  https://www.nature.com/articles/s42256-023-007...   \n",
       "\n",
       "                                            TweetURL  \\\n",
       "0  https://x.com/zhangir_azerbay/status/171409802...   \n",
       "1  https://x.com/omarsar0/status/1713940983199506...   \n",
       "2  https://x.com/AkariAsai/status/171511027707796...   \n",
       "3  https://x.com/omarsar0/status/1714986431859282...   \n",
       "4  https://x.com/AIatMeta/status/1715041427283902...   \n",
       "\n",
       "                                            Abstract  \n",
       "0  We present Llemma, a large language model for ...  \n",
       "1  This paper provides a survey of the emerging a...  \n",
       "2  Despite their remarkable capabilities, large l...  \n",
       "3  We present a study of retrieval-augmented lang...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to list of dicts with Title and Description columns only\n",
    "\n",
    "ml_papers_dict = ml_papers.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Title': 'Llemma',\n",
       "  'Description': 'an LLM for mathematics which is based on continued pretraining from Code Llama on the Proof-Pile-2 dataset; the dataset involves scientific paper, web data containing mathematics, and mathematical code; Llemma outperforms open base models and the unreleased Minerva on the MATH benchmark; the model is released, including dataset and code to replicate experiments.',\n",
       "  'PaperURL': 'https://arxiv.org/abs/2310.10631',\n",
       "  'TweetURL': 'https://x.com/zhangir_azerbay/status/1714098025956864031?s=20',\n",
       "  'Abstract': 'We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.'},\n",
       " {'Title': 'LLMs for Software Engineering',\n",
       "  'Description': 'a comprehensive survey of LLMs for software engineering, including open research and technical challenges.',\n",
       "  'PaperURL': 'https://arxiv.org/abs/2310.03533',\n",
       "  'TweetURL': 'https://x.com/omarsar0/status/1713940983199506910?s=20',\n",
       "  'Abstract': \"This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.\"}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_papers_dict[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        batch_embeddings = embedding_model.encode(input)\n",
    "        return batch_embeddings.tolist()\n",
    "\n",
    "embed_fn = MyEmbeddingFunction()\n",
    "\n",
    "# Initialize the chromadb directory, and client.\n",
    "client = chromadb.PersistentClient(path=\"./chromadb\")\n",
    "\n",
    "# create collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=f\"ml-papers-nov-2023\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:25<00:00,  2.81s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings, and index titles in batches\n",
    "batch_size = 50\n",
    "\n",
    "# loop through batches and generate + store embeddings\n",
    "for i in tqdm(range(0, len(ml_papers_dict), batch_size)):\n",
    "\n",
    "    i_end = min(i + batch_size, len(ml_papers_dict))\n",
    "    batch = ml_papers_dict[i : i + batch_size]\n",
    "\n",
    "    # Replace title with \"No Title\" if empty string\n",
    "    batch_titles = [str(paper[\"Title\"]) if str(paper[\"Title\"]) != \"\" else \"No Title\" for paper in batch]\n",
    "    batch_ids = [str(sum(ord(c) + random.randint(1, 10000) for c in paper[\"Title\"])) for paper in batch]\n",
    "    batch_metadata = [\n",
    "        dict(\n",
    "            url=paper[\"PaperURL\"],\n",
    "            abstract=paper['Abstract']\n",
    "        )\n",
    "        for paper in batch\n",
    "    ]\n",
    "\n",
    "    # generate embeddings\n",
    "    # batch_embeddings = embedding_model.encode(batch_titles)\n",
    "    batch_embeddings = embedding_model.encode([paper['Abstract'] for paper in batch])\n",
    "\n",
    "    # upsert to chromadb\n",
    "    collection.upsert(\n",
    "        ids=batch_ids,\n",
    "        metadatas=batch_metadata,\n",
    "        documents=batch_titles,\n",
    "        embeddings=batch_embeddings.tolist(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['LLMs for Software Engineering', 'Communicative Agents for Software Development', 'LLMs for Software Engineering', 'LLMs for Software Engineering', 'Generative AI for Programming Education']]\n"
     ]
    }
   ],
   "source": [
    "collection = client.get_or_create_collection(\n",
    "    name=f\"ml-papers-nov-2023\",\n",
    "    embedding_function=embed_fn\n",
    ")\n",
    "\n",
    "retriever_results = collection.query(\n",
    "    query_texts=[\"Software Engineering\"],\n",
    "    n_results=5,\n",
    ")\n",
    "\n",
    "print(retriever_results[\"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'abstract': 'How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\\\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\\\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\\\url{this https URL}.',\n",
       "   'url': 'https://arxiv.org/abs/2304.01373'},\n",
       "  {'abstract': \"Over the last decades, excellent computational chemistry tools have been developed. Integrating them into a single platform with enhanced accessibility could help reaching their full potential by overcoming steep learning curves. Recently, large-language models (LLMs) have shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientific applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 18 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our agent autonomously planned and executed the syntheses of an insect repellent, three organocatalysts, and guided the discovery of a novel chromophore. Our evaluation, including both LLM and expert assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks. Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow's performance. Our work not only aids expert chemists and lowers barriers for non-experts, but also fosters scientific advancement by bridging the gap between experimental and computational chemistry.\",\n",
       "   'url': 'https://arxiv.org/abs/2304.05376'},\n",
       "  {'abstract': 'Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.',\n",
       "   'url': 'https://arxiv.org/abs/2303.18223'},\n",
       "  {'abstract': 'We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla70B and PaLM-540B. We release all our models to the research community.',\n",
       "   'url': 'https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/'},\n",
       "  {'abstract': 'We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: this https URL.',\n",
       "   'url': 'https://arxiv.org/abs/2301.00774'},\n",
       "  {'abstract': 'We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.',\n",
       "   'url': 'https://arxiv.org/abs/2301.12652'},\n",
       "  {'abstract': 'We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at this https URL.',\n",
       "   'url': 'https://arxiv.org/abs/2303.16199'},\n",
       "  {'abstract': 'Large language models (LLMs) represent a major advance in artificial intelligence (AI) research. However, the widespread use of LLMs is also coupled with significant ethical and social challenges. Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks. In this article, we address that gap by outlining a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training but prior to their release), and application audits (of applications based on LLMs) complement and inform each other. We show how audits, when conducted in a structured and coordinated manner on all three levels, can be a feasible and effective mechanism for identifying and managing some of the ethical and social risks posed by LLMs. However, it is important to remain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing LLMs at all. Ultimately, this article seeks to expand the methodological toolkit available to technology providers and policymakers who wish to analyse and evaluate LLMs from technical, ethical, and legal perspectives.',\n",
       "   'url': 'https://arxiv.org/abs/2302.08500'},\n",
       "  {'abstract': 'Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.',\n",
       "   'url': 'https://arxiv.org/abs/2305.17333'},\n",
       "  {'abstract': \"Large language models (LLMs) have emerged as valuable tools for many natural language understanding tasks. In safety-critical applications such as healthcare, the utility of these models is governed by their ability to generate outputs that are factually accurate and complete. In this work, we present dialog-enabled resolving agents (DERA). DERA is a paradigm made possible by the increased conversational abilities of LLMs, namely GPT-4. It provides a simple, interpretable forum for models to communicate feedback and iteratively improve output. We frame our dialog as a discussion between two agent types - a Researcher, who processes information and identifies crucial problem components, and a Decider, who has the autonomy to integrate the Researcher's information and makes judgments on the final output.\\nWe test DERA against three clinically-focused tasks. For medical conversation summarization and care plan generation, DERA shows significant improvement over the base GPT-4 performance in both human expert preference evaluations and quantitative metrics. In a new finding, we also show that GPT-4's performance (70%) on an open-ended version of the MedQA question-answering (QA) dataset (Jin et al. 2021, USMLE) is well above the passing level (60%), with DERA showing similar performance. We release the open-ended MEDQA dataset at this https URL.\",\n",
       "   'url': 'https://arxiv.org/abs/2303.17071'}]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user query\n",
    "user_query = \"S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models\"\n",
    "\n",
    "# query for user query\n",
    "results = collection.query(\n",
    "    query_texts=[user_query],\n",
    "    n_results=10,\n",
    ")\n",
    "\n",
    "results['metadatas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# concatenate titles into a single string\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m short_titles \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mresults\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124m[INST]\u001b[39m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mYour main task is to generate 5 SUGGESTED_TITLES based for the PAPER_TITLE\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m[/INST]\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     19\u001b[0m responses \u001b[38;5;241m=\u001b[39m get_completion(prompt_template, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-7b-instruct-4k\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# concatenate titles into a single string\n",
    "short_titles = '\\n'.join(results['documents'][0])\n",
    "\n",
    "prompt_template = f'''[INST]\n",
    "\n",
    "Your main task is to generate 5 SUGGESTED_TITLES based for the PAPER_TITLE\n",
    "\n",
    "You should mimic a similar style and length as SHORT_TITLES but PLEASE DO NOT include titles from SHORT_TITLES in the SUGGESTED_TITLES, only generate versions of the PAPER_TITLE.\n",
    "\n",
    "PAPER_TITLE: {user_query}\n",
    "\n",
    "SHORT_TITLES: {short_titles}\n",
    "\n",
    "SUGGESTED_TITLES:\n",
    "\n",
    "[/INST]\n",
    "'''\n",
    "\n",
    "responses = get_completion(prompt_template, model=\"mistral-7b-instruct-4k\", max_tokens=2000)\n",
    "suggested_titles = ''.join([str(r) for r in responses])\n",
    "\n",
    "# Print the suggestions.\n",
    "print(\"Model Suggestions:\")\n",
    "print(suggested_titles)\n",
    "print(\"\\n\\n\\nPrompt Template:\")\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A Survey on LLM-based Autonomous Agents',\n",
       "  'AutoRobotics-Zero',\n",
       "  'Reflexion: an autonomous agent with dynamic memory and self-reflection',\n",
       "  'A Cookbook of Self-Supervised Learning',\n",
       "  'Robot Parkour Learning',\n",
       "  'Robot Parkour Learning',\n",
       "  'Emergence of Maps in the Memories of Blind Navigation Agents',\n",
       "  'Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control',\n",
       "  'Self-Check',\n",
       "  'AutoMix']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user query\n",
    "user_query = \"Self-driving car autonomous vehicle\"\n",
    "\n",
    "# query for user query\n",
    "results = collection.query(\n",
    "    query_texts=[user_query],\n",
    "    n_results=10,\n",
    ")\n",
    "\n",
    "results['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. S3Eval: A Comprehensive Evaluation Suite for Large Language Models\\n2. Synthetic and Scalable Evaluation for Large Language Models\\n3. Systematic Evaluation of Large Language Models with S3Eval\\n4. S3Eval: A Synthetic and Scalable Approach to Language Model Evaluation\\n5. S3Eval: A Synthetic and Scalable Evaluation Suite for Large Language Models'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      " {\n",
      "  \"SEARCH_STRING\": \"language model document retrieval\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_query = {\"USER_QUERY\": \"What is a language model that can find relevant documents given a text prompt?\"}\n",
    "\n",
    "prompt_template = f\"\"\"[INST] Please generate a single keyword-based search string in JSON format (with the key \"SEARCH_STRING\") based on the following user query:\n",
    "{json.dumps(user_query)} [INST]\"\"\"\n",
    "\n",
    "response = get_completion(prompt_template, model=\"mistral-7b-instruct-4k\", max_tokens=2000)\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SEARCH_STRING': 'language model document retrieval'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_json = response[response.index('{'):]\n",
    "response_json = json.loads(response_json)\n",
    "response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results:\n",
      "[['Rethinking with Retrieval: Faithful Large Language Model Inference', 'REPLUG: Retrieval-Augmented Black-Box Language Models', 'Long-range Language Modeling with Self-Retrieval', 'REPLUG: Retrieval-Augmented Black-Box Language Models', 'REPLUG: Retrieval-Augmented Black-Box Language Models']]\n",
      "\n",
      "Prompt:\n",
      "[INST] Using your knowledge and the following 5 documents, please answer the USER_QUERY to the best of your ability if possible. The information is provided below in JSON format.\n",
      "\n",
      "{\"USER_QUERY\": {\"USER_QUERY\": \"What is a language model that can find relevant documents given a text prompt?\"}, \"DOCUMENTS\": [{\"title\": \"Rethinking with Retrieval: Faithful Large Language Model Inference\", \"abstract\": \"Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.\"}, {\"title\": \"REPLUG: Retrieval-Augmented Black-Box Language Models\", \"abstract\": \"We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.\"}, {\"title\": \"Long-range Language Modeling with Self-Retrieval\", \"abstract\": \"Retrieval-augmented language models (LMs) have received much attention recently. However, typically the retriever is not trained jointly as a native component of the LM, but added to an already-pretrained LM, which limits the ability of the LM and the retriever to adapt to one another. In this work, we propose the Retrieval-Pretrained Transformer (RPT), an architecture and training procedure for jointly training a retrieval-augmented LM from scratch for the task of modeling long texts. Given a recently generated text chunk in a long document, the LM computes query representations, which are then used to retrieve earlier chunks in the document, located potentially tens of thousands of tokens before. Information from retrieved chunks is fused into the LM representations to predict the next target chunk. We train the retriever component with a semantic objective, where the goal is to retrieve chunks that increase the probability of the next chunk, according to a reference LM. We evaluate RPT on four long-range language modeling tasks, spanning books, code, and mathematical writing, and demonstrate that RPT improves retrieval quality and subsequently perplexity across the board compared to strong baselines.\"}, {\"title\": \"REPLUG: Retrieval-Augmented Black-Box Language Models\", \"abstract\": \"We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.\"}, {\"title\": \"REPLUG: Retrieval-Augmented Black-Box Language Models\", \"abstract\": \"We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.\"}]}\n",
      "[/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    # query_texts=[\"RAG LLM text retrieval search\"],\n",
    "    query_texts=[response_json['SEARCH_STRING']],\n",
    "    n_results=5,\n",
    ")\n",
    "\n",
    "documents = []\n",
    "for i in range(5):\n",
    "    document = {\n",
    "        \"title\": results['documents'][0][i],\n",
    "        \"abstract\": results['metadatas'][0][i]['abstract']\n",
    "    }\n",
    "    documents.append(document)\n",
    "\n",
    "prompt_json = {\n",
    "    \"USER_QUERY\": user_query,\n",
    "    \"DOCUMENTS\": documents\n",
    "}\n",
    "\n",
    "prompt_template = \\\n",
    "f'''[INST] Using your knowledge and the following 5 documents, please answer the USER_QUERY to the best of your ability if possible. The information is provided below in JSON format.\n",
    "\n",
    "{json.dumps(prompt_json)}\n",
    "[/INST]\n",
    "'''\n",
    "\n",
    "print(\"Search results:\")\n",
    "print(results['documents'])\n",
    "\n",
    "print(\"\\nPrompt:\")\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "\n",
      "A language model that can find relevant documents given a text prompt is a retrieval-augmented language model. Retrieval-augmented language models use external knowledge to assist language models in making predictions. The external knowledge is retrieved based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. One example of a retrieval-augmented language model is REPLUG, which treats the language model as a black box and augments it with a tuneable retrieval model. REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. The LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt_template, model=\"mistral-7b-instruct-4k\", max_tokens=2000)\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
